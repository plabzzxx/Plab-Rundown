"""
å®Œæ•´å·¥ä½œæµ - ä»é‚®ä»¶è·å–åˆ°å¾®ä¿¡è‰ç¨¿ç®±æ¨é€
åŒ…å«æ‰€æœ‰æ­¥éª¤ï¼šè·å–é‚®ä»¶ -> å‰ªåˆ‡ -> ç¿»è¯‘ -> æ ¼å¼åŒ– -> æ¨é€åˆ°å¾®ä¿¡
"""

import sys
import os
from pathlib import Path
from dotenv import load_dotenv
import yaml
import re
from datetime import datetime
import pytz
import requests
from bs4 import BeautifulSoup, NavigableString

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from src.gmail.client import GmailClient
from src.gmail.parser import EmailParser
from src.translator.langchain_translator import LangChainTranslator
from src.wechat.table_based_converter import TableBasedConverter
from src.wechat.publisher import WeChatPublisher
from src.utils.logger import setup_logging, get_logger
from src.utils.config import get_config

# åˆå§‹åŒ–æ—¥å¿—
setup_logging(log_level="INFO", log_file="logs/app.log")
logger = get_logger(__name__)


def download_image(url: str, save_path: Path) -> bool:
    """ä¸‹è½½å›¾ç‰‡"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'wb') as f:
            f.write(response.content)
        return True
    except Exception as e:
        logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
        return False


def clean_greeting(html_content: str) -> str:
    """æ¸…ç†æ¬¢è¿è¯­ä¸­çš„ä¸ªäººç§°å‘¼"""
    patterns = [
        r'Good morning,\s+\w+\.',
        r'Good afternoon,\s+\w+\.',
        r'Good evening,\s+\w+\.',
        r'Hello,\s+\w+\.',
        r'Hi,\s+\w+\.',
    ]
    for pattern in patterns:
        html_content = re.sub(
            pattern,
            lambda m: m.group(0).split(',')[0] + '.',
            html_content,
            flags=re.IGNORECASE
        )
    return html_content


def get_title_with_prefix(original_title: str) -> str:
    """ä¸ºæ ‡é¢˜æ·»åŠ æ—¥æœŸå‰ç¼€"""
    config_path = Path("config/config.yaml")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
            title_prefix_template = yaml_config.get('wechat', {}).get('title_prefix', 'ã€{date}AIæ—©æŠ¥ã€‘')
    else:
        title_prefix_template = 'ã€{date}AIæ—©æŠ¥ã€‘'

    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(beijing_tz)
    date_str = now.strftime('%mæœˆ%dæ—¥').lstrip('0').replace('æœˆ0', 'æœˆ')
    title_prefix = title_prefix_template.replace('{date}', date_str)
    return f"{title_prefix}{original_title}"


def extract_title_and_digest(html_content: str) -> tuple:
    """ä»HTMLä¸­æå–æ ‡é¢˜å’Œæ‘˜è¦"""
    import emoji

    soup = BeautifulSoup(html_content, 'html.parser')

    # æå–ç¬¬ä¸€ä¸ªh3ä½œä¸ºæ ‡é¢˜
    title_elem = soup.find('h3')
    title = title_elem.get_text(strip=True) if title_elem else "AIæ—©æŠ¥"

    # å»é™¤emoji
    title = emoji.replace_emoji(title, '').strip()

    # æå–ç¬¬ä¸€ä¸ªæœ‰æ–‡æœ¬å†…å®¹çš„æ®µè½ä½œä¸ºæ‘˜è¦(è·³è¿‡bannerå›¾ç‰‡çš„pæ ‡ç­¾)
    digest = ""
    for p in soup.find_all('p'):
        # è·³è¿‡åªåŒ…å«å›¾ç‰‡çš„pæ ‡ç­¾
        if p.find('img') and not p.get_text(strip=True):
            continue
        text = p.get_text(strip=True)
        if text:
            digest = text
            break

    # é™åˆ¶æ‘˜è¦é•¿åº¦
    if len(digest) > 100:
        digest = digest[:97] + "..."

    return title, digest


def main():
    """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ: è·å–é‚®ä»¶ -> å‰ªåˆ‡ -> ç¿»è¯‘ -> æ ¼å¼åŒ– -> æ¨é€åˆ°å¾®ä¿¡"""
    import sys
    import io

    # è®¾ç½®stdoutä¸ºUTF-8ç¼–ç ,é¿å…Windowsæ§åˆ¶å°emojié”™è¯¯
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("=" * 70)
    print("ğŸš€ Plab-Rundown å®Œæ•´å·¥ä½œæµ")
    print("=" * 70)
    print()

    try:
        # åŠ è½½é…ç½®
        config = get_config()

        # ============================================
        # æ­¥éª¤ 1: ä¸‹è½½æœ€æ–°é‚®ä»¶
        # ============================================
        print("ğŸ“¥ æ­¥éª¤ 1: ä¸‹è½½æœ€æ–°é‚®ä»¶")
        # å¼€å§‹æ—¶é—´
        start_time = datetime.now()
        print("å¼€å§‹æ—¶é—´:", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        print("-" * 70)

        gmail_client = GmailClient(
            credentials_path=config.gmail_credentials_path,
            token_path=config.gmail_token_path
        )
        parser = EmailParser()

        # è·å–æœ€æ–°é‚®ä»¶
        logger.info(f"æ­£åœ¨è·å–æ¥è‡ª {config.sender_email} çš„æœ€æ–°é‚®ä»¶...")
        message = gmail_client.get_latest_email(
            sender=config.sender_email,
            days_back=7
        )

        if not message:
            logger.error("âŒ æœªæ‰¾åˆ°é‚®ä»¶")
            print("\nâŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„é‚®ä»¶")
            print("è¯·æ£€æŸ¥:")
            print("  1. Gmail API å‡­æ®æ˜¯å¦æ­£ç¡®")
            print(f"  2. æ˜¯å¦æœ‰æ¥è‡ª {config.sender_email} çš„é‚®ä»¶")
            print("  3. é‚®ä»¶æ˜¯å¦åœ¨æœ€è¿‘7å¤©å†…")
            return

        # æå–é‚®ä»¶æ•°æ®
        email_data = gmail_client.extract_email_data(message)

        logger.info("âœ… æˆåŠŸè·å–é‚®ä»¶")
        logger.info(f"ğŸ“§ ä¸»é¢˜: {email_data['subject']}")
        logger.info(f"ğŸ‘¤ å‘ä»¶äºº: {email_data['sender']}")
        logger.info(f"ğŸ“… æ—¥æœŸ: {email_data['date']}")

        print("âœ… æˆåŠŸè·å–é‚®ä»¶")
        print(f"ğŸ“§ ä¸»é¢˜: {email_data['subject']}")
        print(f"ğŸ‘¤ å‘ä»¶äºº: {email_data['sender']}")
        print(f"ğŸ“… æ—¥æœŸ: {email_data['date']}")
        print()

        # è·å–HTMLå†…å®¹
        html_content = gmail_client.get_email_html(email_data['id'])

        if not html_content:
            logger.error("âŒ æ— æ³•è§£æé‚®ä»¶å†…å®¹")
            print("\nâŒ æ— æ³•è§£æé‚®ä»¶å†…å®¹")
            return

        logger.info(f"âœ… é‚®ä»¶å†…å®¹å¤§å°: {len(html_content)} å­—ç¬¦")
        print(f"âœ… é‚®ä»¶å†…å®¹å¤§å°: {len(html_content)} å­—ç¬¦")
        print()

        # ä¿å­˜åŸå§‹HTML
        parser.save_html_to_file(html_content, "original_email", "data")
        print(f"ğŸ’¾ åŸå§‹é‚®ä»¶å·²ä¿å­˜: data/original_email.html")
        print()

        # ============================================
        # æ­¥éª¤ 2: å‰ªåˆ‡é‚®ä»¶å†…å®¹
        # ============================================
        print("âœ‚ï¸  æ­¥éª¤ 2: å‰ªåˆ‡é‚®ä»¶å†…å®¹")
        print("-" * 70)

        logger.info("æ­£åœ¨å‰ªåˆ‡é‚®ä»¶å†…å®¹...")
        clipped_html = parser.clip_email_html(html_content)

        logger.info(f"âœ… å‰ªåˆ‡åå†…å®¹å¤§å°: {len(clipped_html)} å­—ç¬¦")
        print(f"âœ… å‰ªåˆ‡åå†…å®¹å¤§å°: {len(clipped_html)} å­—ç¬¦")
        print()

        # ä¿å­˜å‰ªåˆ‡åçš„HTML
        parser.save_html_to_file(clipped_html, "clipped_email", "data")
        print(f"ğŸ’¾ å‰ªåˆ‡åé‚®ä»¶å·²ä¿å­˜: data/clipped_email.html")
        print()

        # ============================================
        # æ­¥éª¤ 3: æ¸…ç†æ¬¢è¿è¯­å¹¶ç¿»è¯‘
        # ============================================
        print("ğŸŒ æ­¥éª¤ 3: ç¿»è¯‘å†…å®¹")
        print("-" * 70)

        logger.info("æ¸…ç†æ¬¢è¿è¯­ä¸­çš„ä¸ªäººç§°å‘¼...")
        clipped_html = clean_greeting(clipped_html)
        logger.info("âœ… æ¬¢è¿è¯­æ¸…ç†å®Œæˆ")
        print("âœ… æ¬¢è¿è¯­æ¸…ç†å®Œæˆ")
        print()

        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        logger.info("åˆå§‹åŒ–ç¿»è¯‘å™¨...")
        translator = LangChainTranslator()
        print("âœ… ç¿»è¯‘å™¨åˆå§‹åŒ–å®Œæˆ")
        print()

        # åˆ†å—ç¿»è¯‘
        logger.info("å¼€å§‹ç¿»è¯‘é‚®ä»¶å†…å®¹...")
        soup = BeautifulSoup(clipped_html, 'html.parser')

        # æ‰¾åˆ°æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        text_nodes = []
        for elem in soup.find_all(string=True):
            if elem.parent.name in ['script', 'style', '[document]', 'head', 'title', 'meta']:
                continue
            text = str(elem).strip()
            if text and len(text) > 3 and any(c.isalpha() for c in text):
                text_nodes.append(elem)

        logger.info(f"ğŸ“ æ‰¾åˆ° {len(text_nodes)} ä¸ªéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬èŠ‚ç‚¹")
        print(f"ğŸ“ æ‰¾åˆ° {len(text_nodes)} ä¸ªéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬èŠ‚ç‚¹")
        print()

        # å›ºå®šæ ‡é¢˜æ˜ å°„
        fixed_titles = {
            "LATEST DEVELOPMENTS": os.getenv("SECTION_TITLE_LATEST_DEVELOPMENTS", "ä»Šæ—¥è¦é—»"),
            "QUICK HITS": os.getenv("SECTION_TITLE_QUICK_HITS", "å…¶ä»–è¦é—»"),
            "Trending AI Tools": os.getenv("SUBSECTION_TITLE_TRENDING_TOOLS", "ğŸ› ï¸ çƒ­é—¨ AI å·¥å…·"),
            "Everything else in AI today": os.getenv("SUBSECTION_TITLE_EVERYTHING_ELSE", "ğŸ“° ä»Šå¤©äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…¶ä»–ä¸€åˆ‡"),
        }

        # ç¿»è¯‘æ¯ä¸ªæ–‡æœ¬èŠ‚ç‚¹
        for i, text_node in enumerate(text_nodes, 1):
            original_text = str(text_node).strip()

            if i % 10 == 0 or i == 1:
                logger.info(f"[{i}/{len(text_nodes)}] ç¿»è¯‘ä¸­...")
                print(f"[{i}/{len(text_nodes)}] ç¿»è¯‘ä¸­...")

            # æ£€æŸ¥æ˜¯å¦æ˜¯å›ºå®šæ ‡é¢˜
            if original_text in fixed_titles:
                translated_text = fixed_titles[original_text]
                logger.info(f"ä½¿ç”¨å›ºå®šç¿»è¯‘: {original_text} -> {translated_text}")
            else:
                translated_text = translator.translate(original_text)

            text_node.replace_with(NavigableString(translated_text))

        translated_html = str(soup)
        logger.info("âœ… ç¿»è¯‘å®Œæˆ")
        print("âœ… ç¿»è¯‘å®Œæˆ")
        print()

        # ä¿å­˜ç¿»è¯‘åçš„HTML
        parser.save_html_to_file(translated_html, "translated_email", "data")
        print(f"ğŸ’¾ ç¿»è¯‘åé‚®ä»¶å·²ä¿å­˜: data/translated_email.html")
        print()

        # ============================================
        # æ­¥éª¤ 4: æ ¼å¼åŒ–ä¸ºå¾®ä¿¡å…¬ä¼—å·æ ¼å¼
        # ============================================
        print("ğŸ“ æ­¥éª¤ 4: æ ¼å¼åŒ–ä¸ºå¾®ä¿¡å…¬ä¼—å·æ ¼å¼")
        print("-" * 70)

        # ä»YAMLé…ç½®è¯»å–æ˜¯å¦è‡ªåŠ¨å‘å¸ƒ
        auto_publish = False
        config_path = Path("config/config.yaml")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                auto_publish = yaml_config.get('wechat', {}).get('auto_publish', False)

        logger.info("åˆå§‹åŒ–å¾®ä¿¡å‘å¸ƒå™¨...")
        publisher = WeChatPublisher(auto_publish=auto_publish)
        formatter = TableBasedConverter(publisher=publisher)

        logger.info("å¼€å§‹æ ¼å¼åŒ–...")
        formatted_html = formatter.convert(translated_html)
        logger.info(f"âœ… æ ¼å¼åŒ–å®Œæˆ")
        print("âœ… æ ¼å¼åŒ–å®Œæˆ")
        print()

        # ä¿å­˜æ ¼å¼åŒ–åçš„HTML
        parser.save_html_to_file(formatted_html, "wechat_formatted", "data")
        print(f"ğŸ’¾ æ ¼å¼åŒ–åé‚®ä»¶å·²ä¿å­˜: data/wechat_formatted.html")
        print()

        # ============================================
        # æ­¥éª¤ 5: æ¨é€åˆ°å¾®ä¿¡å…¬ä¼—å·
        # ============================================
        print("ğŸ“¤ æ­¥éª¤ 5: æ¨é€åˆ°å¾®ä¿¡å…¬ä¼—å·")
        print("-" * 70)

        # æå–æ ‡é¢˜å’Œæ‘˜è¦
        title, digest = extract_title_and_digest(formatted_html)
        title = get_title_with_prefix(title)

        logger.info(f"æ ‡é¢˜: {title}")
        logger.info(f"æ‘˜è¦: {digest}")
        print(f"ğŸ“Œ æ ‡é¢˜: {title}")
        print(f"ğŸ“ æ‘˜è¦: {digest}")
        print()

        # æå–ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºå°é¢
        soup = BeautifulSoup(formatted_html, 'html.parser')
        first_img = soup.find('img')

        thumb_media_id = None
        if first_img:
            img_url = first_img.get('src', '')
            if img_url and 'http' in img_url:
                logger.info(f"æ‰¾åˆ°å°é¢å›¾ç‰‡: {img_url[:80]}...")
                print(f"ğŸ–¼ï¸  æ‰¾åˆ°å°é¢å›¾ç‰‡")

                # ä¸‹è½½å›¾ç‰‡
                temp_thumb_path = Path("data/assets/temp_thumb.jpg")
                if download_image(img_url, temp_thumb_path):
                    # ä¸Šä¼ å°é¢å›¾
                    logger.info("ä¸Šä¼ å°é¢å›¾...")
                    thumb_media_id = publisher.upload_thumb_image(str(temp_thumb_path))
                    logger.info(f"âœ… å°é¢å›¾ä¸Šä¼ æˆåŠŸ")
                    print(f"âœ… å°é¢å›¾ä¸Šä¼ æˆåŠŸ")

        # ä»Configè¯»å–ä½œè€…åç§°
        from src.utils.config import Config
        config = Config()
        author = config.wechat_author

        print()
        logger.info("å‘å¸ƒæ–‡ç« åˆ°å¾®ä¿¡å…¬ä¼—å·...")
        print("ğŸ“¤ å‘å¸ƒæ–‡ç« åˆ°å¾®ä¿¡å…¬ä¼—å·...")

        # å‘å¸ƒæ–‡ç« 
        result = publisher.publish_article(
            title=title,
            content=formatted_html,
            author=author,
            digest=digest,
            thumb_media_id=thumb_media_id
        )

        print()
        print("=" * 70)
        if result.get('status') == 'published':
            logger.info("ğŸ‰ æ–‡ç« å‘å¸ƒæˆåŠŸ!")
            logger.info(f"Media ID: {result.get('media_id')}")
            logger.info(f"Publish ID: {result.get('publish_id')}")
            print("ğŸ‰ æ–‡ç« å‘å¸ƒæˆåŠŸ!")
            print(f"Media ID: {result.get('media_id')}")
            print(f"Publish ID: {result.get('publish_id')}")
        else:
            logger.info("âœ… æ–‡ç« å·²ä¿å­˜ä¸ºè‰ç¨¿!")
            logger.info(f"Media ID: {result.get('media_id')}")
            print("âœ… æ–‡ç« å·²ä¿å­˜ä¸ºè‰ç¨¿!")
            print(f"Media ID: {result.get('media_id')}")
            #ç»“æŸæ—¶é—´ï¼Œç”¨æ—¶
            print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ç”¨æ—¶: {datetime.now() - start_time}")
        print("=" * 70)
        print()

    except Exception as e:
        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

