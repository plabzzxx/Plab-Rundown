"""
å®šæ—¶ä»»åŠ¡ä¸»ç¨‹åº
ç”¨äºåœ¨Renderä¸Šè¿è¡Œå®šæ—¶ä»»åŠ¡
"""
import sys
import os
from pathlib import Path
from dotenv import load_dotenv
import yaml

# è®¾ç½®UTF-8ç¼–ç 
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆoverride=True ç¡®ä¿è¦†ç›–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼‰
load_dotenv(override=True)

from src.scheduler.tasks import TaskScheduler
from src.utils.logger import get_logger
from src.gmail.client import GmailClient
from src.gmail.parser import EmailParser
from src.translator.langchain_translator import LangChainTranslator
from src.wechat.table_based_converter import TableBasedConverter
from src.wechat.publisher import WeChatPublisher
from bs4 import BeautifulSoup
import re
from datetime import datetime
import pytz
import requests

logger = get_logger(__name__)


def download_image(url: str, save_path: Path) -> bool:
    """ä¸‹è½½å›¾ç‰‡"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, 'wb') as f:
            f.write(response.content)
        return True
    except Exception as e:
        logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
        return False


def clean_greeting(html_content: str) -> str:
    """æ¸…ç†æ¬¢è¿è¯­ä¸­çš„ä¸ªäººç§°å‘¼"""
    patterns = [
        r'Good morning,\s+\w+\.',
        r'Good afternoon,\s+\w+\.',
        r'Good evening,\s+\w+\.',
        r'Hello,\s+\w+\.',
        r'Hi,\s+\w+\.',
    ]
    for pattern in patterns:
        html_content = re.sub(
            pattern,
            lambda m: m.group(0).split(',')[0] + '.',
            html_content,
            flags=re.IGNORECASE
        )
    return html_content


def get_title_with_prefix(original_title: str) -> str:
    """ä¸ºæ ‡é¢˜æ·»åŠ æ—¥æœŸå‰ç¼€"""
    config_path = Path("config/config.yaml")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)
            title_prefix_template = yaml_config.get('wechat', {}).get('title_prefix', 'ã€{date}AIæ—©æŠ¥ã€‘')
    else:
        title_prefix_template = 'ã€{date}AIæ—©æŠ¥ã€‘'
    
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(beijing_tz)
    date_str = now.strftime('%mæœˆ%dæ—¥').lstrip('0').replace('æœˆ0', 'æœˆ')
    title_prefix = title_prefix_template.replace('{date}', date_str)
    return f"{title_prefix}{original_title}"


def extract_title_and_digest(html_content: str) -> tuple[str, str]:
    """ä»HTMLä¸­æå–æ ‡é¢˜å’Œæ‘˜è¦"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æå–ç¬¬ä¸€ä¸ªh3ä½œä¸ºæ ‡é¢˜
    title_elem = soup.find('h3')
    title = title_elem.get_text(strip=True) if title_elem else "AIæ—©æŠ¥"
    
    # æå–ç¬¬ä¸€ä¸ªæ®µè½ä½œä¸ºæ‘˜è¦
    first_p = soup.find('p')
    digest = first_p.get_text(strip=True) if first_p else ""
    
    # é™åˆ¶æ‘˜è¦é•¿åº¦
    if len(digest) > 100:
        digest = digest[:97] + "..."
    
    return title, digest


def run_daily_workflow():
    """æ‰§è¡Œæ¯æ—¥å·¥ä½œæµ"""
    logger.info("=" * 70)
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥å·¥ä½œæµ")
    logger.info("=" * 70)
    
    try:
        # ç¬¬ä¸€æ­¥: è·å–æœ€æ–°é‚®ä»¶
        logger.info("\nğŸ“§ ç¬¬ä¸€æ­¥: è·å–æœ€æ–°é‚®ä»¶")
        logger.info("-" * 70)

        # åˆå§‹åŒ– Gmail å®¢æˆ·ç«¯ï¼ˆcredentials_path åœ¨ GitHub Actions ä¸­ä¸éœ€è¦ï¼Œä¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        credentials_path = os.getenv('GMAIL_CREDENTIALS_PATH', 'credentials/credentials.json')
        gmail_client = GmailClient(credentials_path=credentials_path)
        parser = EmailParser()
        
        # ä»é…ç½®è¯»å–å‘ä»¶äºº
        config_path = Path("config/config.yaml")
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                sender_email = yaml_config.get('gmail', {}).get('sender_email', 'news@daily.therundown.ai')
        else:
            sender_email = 'news@daily.therundown.ai'
        
        logger.info(f"æŸ¥æ‰¾å‘ä»¶äºº: {sender_email}")
        # ä½¿ç”¨ get_latest_email æ–¹æ³•ç›´æ¥è·å–æœ€æ–°é‚®ä»¶
        # ç­–ç•¥é€‰é¡¹ï¼š
        # - days_back=1: è·å–æœ€è¿‘1å¤©å†…çš„æœ€æ–°é‚®ä»¶ï¼ˆæ›´ä¸¥æ ¼ï¼Œç¡®ä¿æ˜¯å½“å¤©çš„ï¼‰
        # - days_back=7: è·å–æœ€è¿‘7å¤©å†…çš„æœ€æ–°é‚®ä»¶ï¼ˆæ›´å®½æ¾ï¼Œé¿å…æ¼æ‰é‚®ä»¶ï¼‰
        # email_data = gmail_client.get_latest_email(sender=sender_email, days_back=1)  # åŸç­–ç•¥ï¼šæœ€è¿‘1å¤©
        email_data = gmail_client.get_latest_email(sender=sender_email, days_back=7)  # å½“å‰ç­–ç•¥ï¼šæœ€è¿‘7å¤©

        if not email_data:
            logger.warning("æœªæ‰¾åˆ°é‚®ä»¶,è·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
            return

        logger.info("æˆåŠŸè·å–æœ€æ–°é‚®ä»¶")

        # è§£æé‚®ä»¶å†…å®¹
        parsed_email = parser.parse_email(email_data)
        html_content = parsed_email.get('html')

        if not html_content:
            logger.error("é‚®ä»¶å†…å®¹ä¸ºç©º,è·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
            return

        logger.info(f"âœ… é‚®ä»¶å†…å®¹å¤§å°: {len(html_content)} å­—ç¬¦")

        # ç¬¬äºŒæ­¥: å‰ªåˆ‡é‚®ä»¶å†…å®¹
        logger.info("\nâœ‚ï¸  ç¬¬äºŒæ­¥: å‰ªåˆ‡é‚®ä»¶å†…å®¹")
        logger.info("-" * 70)

        clipped_html = parser.clip_email_html(html_content)
        logger.info(f"âœ… å‰ªåˆ‡åå†…å®¹å¤§å°: {len(clipped_html)} å­—ç¬¦")
        
        # ä¿å­˜å‰ªåˆ‡åçš„HTML
        parser.save_html_to_file(clipped_html, "clipped_email", "data")
        
        # ç¬¬ä¸‰æ­¥: æ¸…ç†æ¬¢è¿è¯­å¹¶ç¿»è¯‘
        logger.info("\nğŸŒ ç¬¬ä¸‰æ­¥: ç¿»è¯‘å†…å®¹")
        logger.info("-" * 70)
        
        logger.info("æ¸…ç†æ¬¢è¿è¯­ä¸­çš„ä¸ªäººç§°å‘¼...")
        clipped_html = clean_greeting(clipped_html)
        logger.info("âœ… æ¬¢è¿è¯­æ¸…ç†å®Œæˆ")
        
        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        translator = LangChainTranslator()
        
        # åˆ†å—ç¿»è¯‘
        from bs4 import NavigableString
        soup = BeautifulSoup(clipped_html, 'html.parser')
        
        # æ‰¾åˆ°æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        text_nodes = []
        for elem in soup.find_all(string=True):
            if elem.parent.name in ['script', 'style', '[document]', 'head', 'title', 'meta']:
                continue
            text = str(elem).strip()
            if text and len(text) > 3 and any(c.isalpha() for c in text):
                text_nodes.append(elem)
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(text_nodes)} ä¸ªéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬èŠ‚ç‚¹")

        # å›ºå®šæ ‡é¢˜æ˜ å°„
        fixed_titles = {
            "LATEST DEVELOPMENTS": os.getenv("SECTION_TITLE_LATEST_DEVELOPMENTS", "ä»Šæ—¥è¦é—»"),
            "QUICK HITS": os.getenv("SECTION_TITLE_QUICK_HITS", "å…¶ä»–è¦é—»"),
            "Trending AI Tools": os.getenv("SUBSECTION_TITLE_TRENDING_TOOLS", "ğŸ› ï¸ çƒ­é—¨ AI å·¥å…·"),
            "Everything else in AI today": os.getenv("SUBSECTION_TITLE_EVERYTHING_ELSE", "ğŸ“° ä»Šå¤©äººå·¥æ™ºèƒ½é¢†åŸŸçš„å…¶ä»–ä¸€åˆ‡"),
        }

        # ç¿»è¯‘æ¯ä¸ªæ–‡æœ¬èŠ‚ç‚¹
        for i, text_node in enumerate(text_nodes, 1):
            original_text = str(text_node).strip()

            if i % 10 == 0 or i == 1:
                logger.info(f"[{i}/{len(text_nodes)}] ç¿»è¯‘ä¸­...")

            # æ£€æŸ¥æ˜¯å¦æ˜¯å›ºå®šæ ‡é¢˜
            if original_text in fixed_titles:
                translated_text = fixed_titles[original_text]
                logger.info(f"ä½¿ç”¨å›ºå®šç¿»è¯‘: {original_text} -> {translated_text}")
            else:
                translated_text = translator.translate(original_text)

            text_node.replace_with(NavigableString(translated_text))
        
        translated_html = str(soup)
        logger.info("âœ… ç¿»è¯‘å®Œæˆ")
        
        # ä¿å­˜ç¿»è¯‘åçš„HTML
        parser.save_html_to_file(translated_html, "translated_email", "data")
        
        # ç¬¬å››æ­¥: æ ¼å¼åŒ–ä¸ºå¾®ä¿¡æ ¼å¼
        logger.info("\nğŸ“ ç¬¬å››æ­¥: æ ¼å¼åŒ–ä¸ºå¾®ä¿¡å…¬ä¼—å·æ ¼å¼")
        logger.info("-" * 70)
        
        # ä»YAMLé…ç½®è¯»å–æ˜¯å¦è‡ªåŠ¨å‘å¸ƒ
        auto_publish = False
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                yaml_config = yaml.safe_load(f)
                auto_publish = yaml_config.get('wechat', {}).get('auto_publish', False)
        
        publisher = WeChatPublisher(auto_publish=auto_publish)
        formatter = TableBasedConverter(publisher=publisher)
        formatted_html = formatter.convert(translated_html)
        
        logger.info(f"âœ… æ ¼å¼åŒ–å®Œæˆ")
        
        # ä¿å­˜æ ¼å¼åŒ–åçš„HTML
        parser.save_html_to_file(formatted_html, "wechat_formatted", "data")
        
        # ç¬¬äº”æ­¥: æ¨é€åˆ°å¾®ä¿¡å…¬ä¼—å·
        logger.info("\nğŸ“¤ ç¬¬äº”æ­¥: æ¨é€åˆ°å¾®ä¿¡å…¬ä¼—å·")
        logger.info("-" * 70)
        
        # æå–æ ‡é¢˜å’Œæ‘˜è¦
        title, digest = extract_title_and_digest(formatted_html)
        title = get_title_with_prefix(title)
        
        logger.info(f"æ ‡é¢˜: {title}")
        logger.info(f"æ‘˜è¦: {digest}")
        
        # æå–ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºå°é¢
        soup = BeautifulSoup(formatted_html, 'html.parser')
        first_img = soup.find('img')
        
        thumb_media_id = None
        if first_img:
            img_url = first_img.get('src', '')
            if img_url and 'http' in img_url:
                logger.info(f"æ‰¾åˆ°å°é¢å›¾ç‰‡: {img_url[:80]}...")
                
                # ä¸‹è½½å›¾ç‰‡
                temp_thumb_path = Path("data/assets/temp_thumb.jpg")
                if download_image(img_url, temp_thumb_path):
                    # ä¸Šä¼ å°é¢å›¾
                    thumb_media_id = publisher.upload_thumb_image(str(temp_thumb_path))
                    logger.info(f"âœ… å°é¢å›¾ä¸Šä¼ æˆåŠŸ")
        
        # ä»Configè¯»å–ä½œè€…åç§°
        from src.utils.config import Config
        config = Config()
        author = config.wechat_author
        
        # å‘å¸ƒæ–‡ç« 
        result = publisher.publish_article(
            title=title,
            content=formatted_html,
            author=author,
            digest=digest,
            thumb_media_id=thumb_media_id
        )
        
        logger.info("=" * 70)
        if result.get('status') == 'published':
            logger.info("ğŸ‰ æ–‡ç« å‘å¸ƒæˆåŠŸ!")
            logger.info(f"Media ID: {result.get('media_id')}")
            logger.info(f"Publish ID: {result.get('publish_id')}")
        else:
            logger.info("ğŸ‰ è‰ç¨¿åˆ›å»ºæˆåŠŸ!")
            logger.info(f"Media ID: {result.get('media_id')}")
            logger.info("âœ… è¯·ç™»å½•å¾®ä¿¡å…¬ä¼—å·åå°æŸ¥çœ‹è‰ç¨¿ç®±")
        logger.info("=" * 70)
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Plab-Rundown å®šæ—¶ä»»åŠ¡å¯åŠ¨")

    # åŠ è½½é…ç½®
    config_path = Path("config/config.yaml")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            scheduler_config = config.get('scheduler', {})
    else:
        scheduler_config = {
            'timezone': 'Asia/Shanghai',
            'cron': {'hour': 9, 'minute': 0}
        }

    # åˆ›å»ºè°ƒåº¦å™¨
    timezone = scheduler_config.get('timezone', 'Asia/Shanghai')
    scheduler = TaskScheduler(timezone=timezone)

    # æ·»åŠ æ¯æ—¥ä»»åŠ¡
    cron_config = scheduler_config.get('cron', {})
    hour = cron_config.get('hour', 9)
    minute = cron_config.get('minute', 0)

    scheduler.add_daily_task(
        task_func=run_daily_workflow,
        hour=hour,
        minute=minute,
        task_id='daily_rundown'
    )

    logger.info(f"âœ… å·²è®¾ç½®æ¯æ—¥ä»»åŠ¡: {hour:02d}:{minute:02d} ({timezone})")

    # å¯åŠ¨è°ƒåº¦å™¨
    scheduler.start()

    # å¯åŠ¨å¥åº·æ£€æŸ¥HTTPæœåŠ¡å™¨ (Renderéœ€è¦)
    port = int(os.getenv('PORT', 10000))
    start_health_server(port, scheduler)

    # ä¿æŒè¿è¡Œ
    logger.info("âœ… è°ƒåº¦å™¨è¿è¡Œä¸­,æŒ‰ Ctrl+C é€€å‡º")
    scheduler.keep_alive()


def start_health_server(port: int, scheduler: TaskScheduler):
    """å¯åŠ¨å¥åº·æ£€æŸ¥HTTPæœåŠ¡å™¨"""
    from http.server import HTTPServer, BaseHTTPRequestHandler
    import threading

    class HealthHandler(BaseHTTPRequestHandler):
        def do_GET(self):
            if self.path == '/health' or self.path == '/':
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()

                # è·å–è°ƒåº¦å™¨çŠ¶æ€
                jobs = scheduler.scheduler.get_jobs()
                status = {
                    'status': 'healthy',
                    'scheduler_running': scheduler.is_running,
                    'jobs_count': len(jobs),
                    'jobs': [
                        {
                            'id': job.id,
                            'name': job.name,
                            'next_run': str(job.next_run_time) if job.next_run_time else None
                        }
                        for job in jobs
                    ]
                }

                import json
                self.wfile.write(json.dumps(status, indent=2).encode())
            else:
                self.send_response(404)
                self.end_headers()

        def log_message(self, format, *args):
            # ç¦ç”¨é»˜è®¤æ—¥å¿—è¾“å‡º
            pass

    def run_server():
        server = HTTPServer(('0.0.0.0', port), HealthHandler)
        logger.info(f"âœ… å¥åº·æ£€æŸ¥æœåŠ¡å™¨å¯åŠ¨: http://0.0.0.0:{port}/health")
        server.serve_forever()

    # åœ¨åå°çº¿ç¨‹è¿è¡ŒHTTPæœåŠ¡å™¨
    thread = threading.Thread(target=run_server, daemon=True)
    thread.start()


if __name__ == "__main__":
    main()

